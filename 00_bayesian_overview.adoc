= Bayesian Analysis - Overview =
:doctype: book
:author: Bharath Kumar S P
:email: bharath030195@gmail.com
:stem: latexmath
:eqnums:
:toc:

== Motivation ==
The game Go is played with human intuition rather than by finding all possible moves and selecting the best. It is hard for a machine to capture those intuitions, so beating a Go champion by an AI machine (from Deep Mind) is considered a breakthrough. Machine learning requires a huge amount of data to reach an expertise level, whereas humans can achieve that with a limited amount of data.

The models should be able to realize when they are confident about their predictions and when they are not, i.e., they should exhibit uncertainties. Models should be able to identify the out-of-distribution data and they should not make wrong predictions with high confidence. We can incorporate these uncertainty modelling abilities into our models through Bayesian modelling.

== Concept Learning ==
We start with a probability distribution over a set of hypothesis (a prior distribution), and keep updating this distribution based on every observation (data) that we receive.

Say we are given a data stem:[D=[16, 8, 2, 64\]]. Predict the hypothesis from which these numbers are drawn. The numbers considered lie from 1 to 100.

* Hypothesis H1: Let's assume that these numbers are from a set of even numbers from 1 to 100. The set of even numbers is stem:[\{2,4,\dots, 100\}], there are 50 elements. Given this hypothesis, the probability of seeing these numbers is stem:[(\frac{1}{50})^4].

* Hypothesis H2: Let's assume that these numbers are from a set of powers of 2 from 1 to 100. The set of numbers is stem:[\{1,2,4,8,16,32,64\}]. Given this hypothesis, the probability of seeing these numbers is stem:[(\frac{1}{7})^4].

The probability of seeing these observations given hypothesis H2 is higher than the probability of seeing these observations given hypothesis H1.

[stem]
++++
P(D| H2) > P(D| H1)
++++

If we assume that these numbers are from a set of powers of 2 except 32, the probability of seeing these numbers is even higher stem:[(\frac{1}{6})^4]. But this is a complex hypothesis (H3), which no one generally thinks of.

Some complex hypotheses end up getting very high likelihood compared to simple hypotheses. So this likelihood alone is not sufficient to infer the underlying true concept. We want to have a mechanism through which we can introduce the domain knowledge that humans possess (giving more weightage to H2 than H3). Bayesian modelling helps us bring that domain knowledge along with the data to solve a problem.

== Bayesian Approach ==

. We start with a prior distribution on the hypotheses. This is done before observing any data. We probably give zero probability to most of the complex hypotheses. The prior distribution on hypotheses is stem:[p(\theta)].
. Obtain the likelihood of observing the given data under a hypothesis. It measures how likely it is to observe the data under a particular hypothesis. It is viewed as a function of hypothesis. stem:[\mathcal{L}(\theta; D) = p(D | \theta)] is the likelihood of the data given the hypothesis.
. Obtain the (unnormalized) posterior distribution by multiplying the prior and likelihood. The posterior distribution over the hypothesis given the data is stem:[p(\theta | D)].

image::.\images\concept_learning.png[align='center', 500, 800]

NOTE: The prior and posterior are probability distributions. They sum (or integrate) to 1. But the likelihood (the plot in the center) is not a probability distribution.

The Bayes' Theorem is

[stem]
++++
p(\theta | D) = \frac{p(D | \theta) p(\theta)}{p(D)}
++++

Bayesian ML: Develop techniques based on Bayesian principles to model data and learn from the data

== Applications ==

Important applications of Bayesian modelling:

* *Active Learning*: In a supervised learning setup, it allows us to choose a subset of data that provides the most useful information to the model to learn the underlying function, instead of training the model on all the data points. This is called active learning where we actively select the data points to train the model instead of randomly providing the data. We sample more training data points from a region where the model is uncertain about its prediction. To do this, we should know where the model is uncertain and it should exhibit uncertainty modelling abilities.

* *Bayesian Optimization*: In some of the optimization problems, it is costly to compute the objective function and we cannot write it as a function of parameters. One such area is hyperparameter tuning where we find the right model by tuning the hyparameter values. Bayesian optimization help us find the optimal value of the hyperparameters without trying out a lot of trials.
