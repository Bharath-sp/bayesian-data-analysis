= Beta-Bernoulli Model =
:doctype: book
:author: Bharath Kumar S P
:email: bharath030195@gmail.com
:stem: latexmath
:eqnums:
:toc:

== Indian Election Example ==
Consider a survey where voters were asked if they will vote for NDA or UPA in the next election.

* Let stem:[X_i] be a binary variable, which is 1 for NDA and 0 for UPA, for stem:[i=1, \dots, N].
* Let stem:[N] be the total number of individuals and stem:[n_d] be the number of individuals who are planning to vote NDA. Say we got the data stem:[N=20] and stem:[n_d=12].

Each stem:[X_i] can be modelled as a Bernoulli random variable, stem:[X_i \sim \text{Bern}(\theta)], where stem:[\theta] is the probability that an individual will vote NDA. The probability mass function of stem:[X_i] is

[stem]
++++
p(x_i \,|\, \theta) = \begin{cases}
\theta & \text{if } x_i=1\\
1-\theta & \text{if } x_i=0
\end{cases} = \theta^{x_i} (1-\theta)^{1-x_i}
++++

=== MLE Estimate ===

[stem]
++++
\begin{align*}
\hat{\theta}_{\text{MLE}} & = \underset{\theta}{\mathrm{arg max}}\, \prod_{i=1}^N p(x_i | \theta) \hspace{1cm} \text{where } \theta \in [0,1] \\
& = \underset{\theta}{\mathrm{arg max}}\, \prod_{i=1}^N \theta^{x_i} (1-\theta)^{1-x_i} \\
& = \underset{\theta}{\mathrm{arg max}}\, \theta^{12} (1-\theta)^8 = 0.6
\end{align*}
++++

In general, for Beta-Bernoulli distribution, the MLE estimate is

[stem]
++++
\hat{\theta}_{\text{MLE}} = \frac{n_d}{N} =  0.6
++++

=== MAP Estimate ===
Let's take our prior as Beta distribution. And say we think that the voters are equally like to vote both the parties. Then we want the peak of the distribution around 0.5. We can equate the mode of the distribution to 0.5 and solve for stem:[a,b] to get an appropriate value of the parameters.

[stem]
++++
\begin{align*}
\mathrm{mode}(\theta) & = \frac{a-1}{a+b-2} = 0.5 \\
& \implies a-1 = 0.5 (a+b-2) \\
\end{align*}
++++

There can be multiple solutions to this equation. If our belief is too strong that the value of stem:[\theta] is 0.5, then we have to consider a larger values of stem:[a] and stem:[b]. Let's consider stem:[a=5, b=5]. Thus stem:[\theta \sim \text{Beta}(5,5)]. Then the PDF of stem:[\theta] is

[stem]
++++
\begin{align*}
    p(\theta) =
    \begin{cases}
    \frac{1}{B(5,5)}{\theta}^4(1-\theta)^4 &\mbox{if } 0 \leq \theta \leq 1 \\
    0 & \mbox{otherwise}
    \end{cases}
   &&\mbox{where } B(5,5) =  \frac{\Gamma(5) \Gamma(5)}{\Gamma(10)}
\end{align*}
++++

The MAP estimate is

[stem]
++++
\begin{align*}
\hat{\theta}_{\text{MAP}} & = \underset{\theta}{\mathrm{arg max}}\, \prod_{i=1}^N p(x_i | \theta) \, p(\theta) \\
& = \underset{\theta}{\mathrm{arg max}}\, \theta^{12} (1-\theta)^8 \cdot \theta^4 (1-\theta)^4 && \text{ignoring the constant } B(5,5)\\
& = \underset{\theta}{\mathrm{arg max}}\, \theta^{16} (1-\theta)^{12}
\end{align*}
++++

Here the prior beta distribution acts as a conjugate prior to the likelihood which is the Bernoulli distribution. Consequently, the posterior distribution will also have the same functional form. Here we see that the posterior distribution is stem:[p(\theta \, |\, \mathcal{D}) \sim \text{Beta}(17,13)].

The MAP estimate is the mode of the posterior distribution, stem:[\hat{\theta}_{\text{MAP}} = \frac{16}{28} \approx 0.57].

In general, for Beta-Bernoulli model, the posterior distribution is stem:[p(\theta \, |\, \mathcal{D}) \sim \text{Beta}(n_d + a,N-n_d + b)]. Then the mode is

[stem]
++++
\hat{\theta}_{\text{MAP}} = \frac{n_d + a -1}{N + a+b-2} = \frac{12 + 5 -1}{20+5+5-2} = \frac{16}{28}
++++

We have stem:[\hat{\theta}_{\text{MLE}} =  0.6]. The mode of the prior distribution is 0.5. We see that the MAP estimate is in between the likelihood and the prior. Depending upon the strength of the prior, it can be closer to the prior or closer to the likelihood.

When we consider an uninformative prior, stem:[\theta \sim \text{Beta}(1,1)], the MAP and the MLE estimate will be the same. Taking mode of the posterior (MAP estimate) doesn't provide any additional information. In such cases, the mean (the Bayesian estimate) can provide a better and robust estimate.

*Inference:*

Having got the MLE and MAP estimates, we can predict stem:[p(\tilde{x}=1 \,|\, \hat{\theta}_{\text{MLE}} = 0.6) = 0.6] and stem:[p(\tilde{x}=1 \,|\, \hat{\theta}_{\text{MAP}} = 0.57) = 0.57]

=== Bayesian Inference ===

We know that the posterior distribution is stem:[p(\theta \, |\, \mathcal{D}) \sim \text{Beta}(17,13)]. We use the full posterior distribution over stem:[\theta] to make predictions.

[stem]
++++
\begin{align*}
p(\tilde{x}=1 \, | \, \mathcal{D}) & = \int_{\theta} p(\tilde{x}=1 \, | \, \theta) \cdot p(\theta \, | \, \mathcal{D} ) d\theta \\
& = \int_{\theta} \theta \cdot p(\theta \, | \, \mathcal{D} ) d\theta \\
& = \frac{1}{B(17,13)} \int_0^1 \theta \cdot \theta^{16} (1-\theta)^{12} d\theta
\end{align*}
++++

We can proceed further to evaluate this integral, but as we know here we are just finding the expectation of stem:[\theta] over the posterior distribution. The posterior distribution is stem:[p(\theta \, |\, \mathcal{D}) \sim \text{Beta}(17,13)]. We can simply find the expected value of this distribution.

====
In general, if stem:[X \sim \text{Beta}(a,b)], the expected value of stem:[X] is

[stem]
++++
\begin{align*}
\mathbb{E}[X] & = \int_x x \cdot x^{a-1} (1-x)^{b-1} \frac{\Gamma(a+b)}{\Gamma{a} \Gamma(b)} dx \\
& = \frac{\Gamma(a+b)}{\Gamma{a} \Gamma(b)} \int_x  x^{a+1}-1 (1-x)^{b-1} dx \\
& = \frac{\Gamma(a+b)}{\Gamma{a} \Gamma(b)} \int_x  x^{a+1}-1 (1-x)^{b-1} dx \\
& = \frac{\Gamma(a+b)}{\Gamma{a} \Gamma(b)} \frac{\Gamma(a+1) \Gamma(b)}{\Gamma{(a+1+b)}} = \frac{\Gamma(a+b)}{\Gamma{a}} 
\frac{a\Gamma(a)}{a+b \cdot \Gamma{(a+b)}} = \frac{a}{a+b}\\
\end{align*}
++++
====

Therefore the expected value of stem:[\text{Beta}(17,13) = \frac{17}{30} = 0.56]. Therefore, as per the Bayesian inference, stem:[p(\tilde{x}=1 \, | \, \mathcal{D}) = 0.56].

== Summary ==

The estimates of stem:[\theta] for Beta-Bernoulli model from various methods are:

[stem]
++++
\begin{align*}
\hat{\theta}_{\text{MLE}} & = \frac{n_d}{N} \\
\hat{\theta}_{\text{MAP}} & = \frac{n_d + a -1}{N + a+b-2} \\
\text{Bayesian } \bar{\theta} & = \frac{a+n_d}{a+b+N}
\end{align*}
++++

== Important Aspects ==

. Under Bayesian framework, we can perform batch-wise or online learning very easily. We can start with a prior, observe the data, and get a posterior distribution. Now this posterior distribution can be considered as the prior, and can be updated as per the new data. The process can be repeated.

. Posterior mean is convex combination of the prior mean stem:[m_1] and the MLE, stem:[\lambda m_1 + (1-\lambda) \hat{\theta}_{\text{MLE}}].